{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some code borrowed from [this tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# selecting one of the 4 gpus randomly\n",
    "device = torch.device(f'cuda:{np.random.randint(4)}')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you get GPU out of memory errors later in the code and you believe your model isn't that heavy, check out the available GPU memory on other devices by invoking the command:\n",
    "\n",
    "```\n",
    "!nvidia-smi\n",
    "```\n",
    "\n",
    "Then, select the suitable GPU (0, 1, 2 or 3). E.g. if you want to select GPU 2, run this:\n",
    "\n",
    "```\n",
    "device = torch.device('cuda:2')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the dataset (CIFAR-10)\n",
    "\n",
    "Here we'll download and open the data from the CIFAR-10 datset. (For more details see [this link](https://pytorch.org/docs/stable/torchvision/datasets.html).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='../../../share/2.8_NetworkRegularization/data/',\n",
    "    train=True, download=True, transform=transform\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='../../../share/2.8_NetworkRegularization/data/',\n",
    "    train=False, download=True, transform=transform\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1024,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = next(iter(trainloader))\n",
    "batch_x = batch_x[:16]\n",
    "batch_y = batch_y[:16]\n",
    "\n",
    "plt.imshow(\n",
    "    batch_x.permute(0, 2, 3, 1).reshape(4, 4, 32, 32, 3).permute(0, 2, 1, 3, 4).reshape(128, 128, 3)\n",
    ")\n",
    "print(\n",
    "    np.array(classes)[batch_y.numpy()].reshape(4, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# We are going to build a model from several convolutional blocks.\n",
    "# I.e. it's going to be:\n",
    "#\n",
    "#       [Conv2d -> Conv2d -> MaxPool2d] x 4\n",
    "#     \n",
    "# So why don't we define such a block as a separate Module?\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,     # <== number of input channels to the 1st convolution\n",
    "                 interm_channels, # <== outputs of the 1st / inputs of the 2nd convolution\n",
    "                 out_channels,    # <== outputs of the 2nd convolution\n",
    "                 use_batchnorm,   # <== whether we'll use batchnorm\n",
    "                 initialization): # <== function that'll initialize the weights\n",
    "        # First we run the base class constructor\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        # And then define all the layers used within a block\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=interm_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=interm_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        if use_batchnorm:\n",
    "            self.bn1 = nn.BatchNorm2d(interm_channels)\n",
    "            self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # If initialization function provided, call it on the weights of the model\n",
    "        if initialization is not None:\n",
    "            initialization(self.conv1.weight)\n",
    "            initialization(self.conv2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        if self.use_batchnorm:\n",
    "            x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        if self.use_batchnorm:\n",
    "            x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "# The model itself:\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, use_batchnorm, initialization):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Convolutional layers:                                         # 3x32x32 (Channels x height x width)\n",
    "        self.conv1 = ConvBlock(3, 8, 16, use_batchnorm, initialization) # -> 8x32x32 -> 16x32x32 -> 16x16x16\n",
    "        self.conv2 = ConvBlock(16, 16, 32, use_batchnorm, initialization) # -> 16x16x16 -> 32x16x16 -> 32x8x8\n",
    "        self.conv3 = ConvBlock(32, 32, 64, use_batchnorm, initialization) # -> 32x8x8 -> 64x8x8 -> 64x4x4\n",
    "        self.conv4 = ConvBlock(64, 64, 128, use_batchnorm, initialization) # -> 64x4x4 -> 128x4x4 -> 128x2x2\n",
    "\n",
    "        # Fully connected layers:\n",
    "        self.fc1 = nn.Linear(128 * 2 * 2, 64)\n",
    "        self.fc2 = nn.Linear(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 10)\n",
    "\n",
    "        # If initialization function provided, call it on the weights of the model\n",
    "        if initialization is not None:\n",
    "            initialization(self.fc1.weight)\n",
    "            initialization(self.fc2.weight)\n",
    "            initialization(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = x.view(x.shape[0], 128 * 2 * 2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # softmax + neg. log likelihood\n",
    "\n",
    "def train_model(model, epochs=3, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # train mode (affects batchnorm layers:\n",
    "                      # in the subsequent forward passes they'll\n",
    "                      # exhibit 'train' behaviour, i.e. they'll\n",
    "                      # normalize activations over batches)\n",
    "        for i, (X, y) in enumerate(tqdm(trainloader)):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        model.eval() # test mode (affects batchnorm layers:\n",
    "                     # in the subsequent forward passes they'll\n",
    "                     # exhibit 'test' behaviour, i.e. they'll\n",
    "                     # use the accumulated running statistics\n",
    "                     # to normalize activations)\n",
    "        epoch_losses = []\n",
    "        epoch_accuracies = []\n",
    "        with torch.no_grad(): # avoid calculating gradients during evaluation\n",
    "            for X, y in testloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                pred = model(X)\n",
    "\n",
    "                epoch_losses.append(loss_fn(pred, y).item())\n",
    "                _, pred = torch.max(pred.data, 1) # pred = index of maximal output along axis=1\n",
    "                epoch_accuracies.append(\n",
    "                    (pred == y).to(torch.float32).mean().item()\n",
    "                )\n",
    "        test_loss.append(np.mean(epoch_losses))\n",
    "        test_accuracy.append(np.mean(epoch_accuracies))\n",
    "\n",
    "    return dict(\n",
    "        train_loss=train_loss,\n",
    "        test_loss=test_loss,\n",
    "        test_accuracy=test_accuracy\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = dict(\n",
    "    fixed_normal_init=dict(\n",
    "        use_batchnorm=False,\n",
    "        initialization=(lambda w: w.data.normal_(std=0.001))\n",
    "    ),\n",
    "    he_normal_init=dict(\n",
    "        use_batchnorm=False,\n",
    "        initialization=(lambda w: torch.nn.init.kaiming_normal_(w, nonlinearity='relu'))\n",
    "    ),\n",
    "    he_normal_init_with_batchnorm=dict(\n",
    "        use_batchnorm=True,\n",
    "        initialization=(lambda w: torch.nn.init.kaiming_normal_(w, nonlinearity='relu'))\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "                                                 # the '**' notation transforms the dictionary\n",
    "                                                 # into keyword arguments, as if we called:\n",
    "result = {                                       # Net(use_batchnorm=config['use_batchnorm'],\n",
    "    name : train_model(Net(**config).to(device)) #     initialization=config['initialization'])\n",
    "    for name, config in configurations.items()\n",
    "} # train the defined configurations, \n",
    "  # get the result as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax0, ax1] = plt.subplots(1, 2, figsize=(12, 4), dpi=100)\n",
    "\n",
    "# per step loss values are too noizy, so we'll use a function to \n",
    "# average them with a running window\n",
    "def running_mean(x, win_size):\n",
    "    return (np.cumsum(x)[win_size:] - np.cumsum(x[:-win_size])) / win_size\n",
    "\n",
    "for (name, metrics), color in zip(result.items(),\n",
    "                                  matplotlib.rcParams['axes.prop_cycle'].by_key()['color']):\n",
    "    ax0.plot(\n",
    "        running_mean(metrics['train_loss'], 20),\n",
    "        color=color, label=name, alpha=0.8\n",
    "    )\n",
    "    ax0.plot(\n",
    "        np.linspace(0, len(metrics['train_loss']), len(metrics['test_loss']) + 1)[1:],\n",
    "        metrics['test_loss'], '--',\n",
    "        color=color, alpha=0.8\n",
    "    )\n",
    "    ax0.set_ylabel(\"Loss\")\n",
    "\n",
    "    ax1.plot(metrics['test_accuracy'], color=color, label=name)\n",
    "    ax1.set_ylabel(\"Test accuracy\")\n",
    "\n",
    "ax1.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn!\n",
    "\n",
    "Try improving the score. Since we don't have too much time to train the model thoroughly, see if you can change the model design and/or use regularization layers (batchnorm, dropout) to get better score quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e6fe7f3a31ce12f5a46b0c67d1ba166",
     "grade": false,
     "grade_id": "e12b7b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError\n",
    "\n",
    "# E.g.:\n",
    "#\n",
    "#     model = ...\n",
    "#     result['MyModel'] = train_model(model, epochs=..., lr=...)\n",
    "#\n",
    "# (and then run the plotting code again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7be9662ab468b54951ed4d60348274bf",
     "grade": true,
     "grade_id": "accuracy65",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model_device = next(model.parameters()).device\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for X, y in testloader:\n",
    "        X, y = X.to(model_device), y.to(model_device)\n",
    "\n",
    "        _, pred = torch.max(model(X).data, 1)\n",
    "\n",
    "        total += len(y)\n",
    "        correct += (pred == y).sum().item()\n",
    "\n",
    "print('accuracy =', correct / total)\n",
    "assert correct / total >= 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57cfc818619784a314a77376f1acc1be",
     "grade": true,
     "grade_id": "accuracy70",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model_device = next(model.parameters()).device\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for X, y in testloader:\n",
    "        X, y = X.to(model_device), y.to(model_device)\n",
    "\n",
    "        _, pred = torch.max(model(X).data, 1)\n",
    "\n",
    "        total += len(y)\n",
    "        correct += (pred == y).sum().item()\n",
    "\n",
    "print('accuracy =', correct / total)\n",
    "assert correct / total >= 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83fa4136636fd982729585af23c52756",
     "grade": true,
     "grade_id": "accuracy75",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model_device = next(model.parameters()).device\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for X, y in testloader:\n",
    "        X, y = X.to(model_device), y.to(model_device)\n",
    "\n",
    "        _, pred = torch.max(model(X).data, 1)\n",
    "\n",
    "        total += len(y)\n",
    "        correct += (pred == y).sum().item()\n",
    "\n",
    "print('accuracy =', correct / total)\n",
    "assert correct / total >= 0.75"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
