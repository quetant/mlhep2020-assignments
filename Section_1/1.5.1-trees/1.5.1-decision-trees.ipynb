{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan for the next 3 seminars\n",
    "* Warm up: decision trees - their strengths and limitations\n",
    "* Moving slowly to real world - bagging ensembles\n",
    "* An interlude on uncertainty\n",
    "* Heavy duty maching learning - gradient boosting machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "![Decision tree joke](https://camo.githubusercontent.com/08b9d1f7280425a77ddf445fb40dc1e0827ea44a/68747470733a2f2f7062732e7477696d672e636f6d2f6d656469612f4231336e3256564349414130684a532e6a7067)\n",
    "\n",
    "// Based on our [course at ICL](https://github.com/yandexdataschool/MLatImperial2019-private/blob/master/04_lab/lab4_trees_ensambling_stacking.ipynb), [MLHEP 2018](https://github.com/yandexdataschool/mlhep2018/blob/master/day1-Mon/seminar-02-tree-ensembles-1-bagging-xgboost-Solution.ipynb), [MLHEP 2019](https://github.com/yandexdataschool/mlhep2019/blob/master/notebooks/day-2/02_decision_trees_and_ensembles.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a toy dataset with 2 features for binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X_toy, y_toy = make_blobs(n_samples=400,\n",
    "                          centers=[[0., 1.], [1., 2.]],\n",
    "                          random_state=14)\n",
    "\n",
    "plt.scatter(X_toy[:, 0], X_toy[:, 1], c=y_toy, alpha=0.8, cmap='bwr')\n",
    "plt.xlabel('X1'), plt.ylabel('X2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key parameters:\n",
    "* `max_depth` – a limit on tree depth (default – no limit)\n",
    "* `min_samples_split` – there should be at least this many samples to split further (default – 2)\n",
    "* `min_samples_leaf` – there should be at least this many samples on one side of a split to consider it valid (default – 1).\n",
    "* `criterion` – the function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib\n",
    "def plot_decision_surface(clf,\n",
    "                          X: np.ndarray,\n",
    "                          y: np.ndarray,\n",
    "                          grid_step: float=0.02,\n",
    "                          cmap='bwr',\n",
    "                          alpha:float=0.6,\n",
    "                          axes=None\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Plot the decision surface of a classifier, visualize selected points\n",
    "    Args:\n",
    "      clf: a fitted model, must support predict method\n",
    "      X[n_examples, n_features]: points where to evaluate the classifier\n",
    "      y[n_examples]: true labels\n",
    "      grid_step: decision surface plottting grid\n",
    "      alpha: opacity of the decision surface\n",
    "      axes(matplotlib.axes._subplots.AxesSubplot): axes where plot, if None, a new figure is created\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the grid\n",
    "    x_top_left = X.min(axis=0) - 1\n",
    "    x_bottom_right = X.max(axis=0) + 1\n",
    "    grid_x0, grid_x1 = np.meshgrid(\n",
    "         np.arange(x_top_left[0], x_bottom_right[0], grid_step),\n",
    "         np.arange(x_top_left[1], x_bottom_right[1], grid_step)\n",
    "      )\n",
    "    \n",
    "    # Calculate predictions on the grid\n",
    "    y_pred_grid = clf.predict(\n",
    "                        np.stack(\n",
    "                              [\n",
    "                                grid_x0.ravel(),\n",
    "                                grid_x1.ravel()\n",
    "                              ],\n",
    "                              axis=1\n",
    "                            )\n",
    "                      ).reshape(grid_x1.shape)\n",
    "    \n",
    "    # Find optimal contour levels and make a filled\n",
    "    # contour plot of predictions\n",
    "    labels = np.sort(np.unique(y))\n",
    "    labels = np.concatenate([[labels[0] - 1],\n",
    "                             labels,\n",
    "                             [labels[-1] + 1]])\n",
    "    medians = (labels[1:] + labels[:-1]) / 2\n",
    "    if axes is None:\n",
    "      _, axes = plt.subplots()\n",
    "    axes.contourf(grid_x0, grid_x1, y_pred_grid, cmap=cmap, alpha=alpha,\n",
    "                 levels=medians)\n",
    "    \n",
    "    # Scatter data points on top of the plot,\n",
    "    # with different styles for correct and wrong\n",
    "    # predictions\n",
    "    y_pred = clf.predict(X)\n",
    "    axes.scatter(*X[y_pred==y].T, c=y[y_pred==y],\n",
    "                marker='o', cmap=cmap, s=10, label='correct')\n",
    "    axes.scatter(*X[y_pred!=y].T, c=y[y_pred!=y],\n",
    "                marker='x', cmap=cmap, s=50, label='errors')\n",
    "\n",
    "    # Dummy plot call to print the accuracy in the legend.\n",
    "    axes.plot([], [], ' ',\n",
    "             label='Accuracy = {:.3f}'.format(accuracy_score(y, y_pred)))\n",
    "    axes.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(min_samples_leaf=30).fit(X_toy, y_toy)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plot_decision_surface(clf, X_toy, y_toy, axes=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As promissed, trees can be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "sklearn.tree.plot_tree(clf, ax=ax, filled=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the multiclassification toy data and then continue to the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_FOLDER = \"../../data/1.5.1-decion-trees\"\n",
    "DATA_FOLDER = \".\"\n",
    "muticlass_toy_data = np.load(os.path.join(DATA_FOLDER, \"data.npz\"))\n",
    "# The data should be in CoCalc. If it isn't, we are soory for that, it can be downloaded here:\n",
    "# https://github.com/yandexdataschool/mlhep2018/raw/master/day1-Mon/data.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toy_multiclass_train, X_toy_multiclass_test, \\\n",
    "  y_toy_multiclass_train, y_toy_multiclass_test = \\\n",
    "    train_test_split(muticlass_toy_data[\"X\"], muticlass_toy_data[\"y\"],\n",
    "                     test_size=0.5, random_state=4)\n",
    "X_toy_train, X_toy_test, y_toy_train, y_toy_test = \\\n",
    "    train_test_split(X_toy, y_toy, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (difficulty: easy)\n",
    "Now it's your turn to investigate how the decision boundary depends on the tree depth. Maximum tree depth is defined by the `max_depth` parameter. Try out the following values: ``[1, 2, 3, 5, 10]``. Make decision boundary plots for both train and test datasets (separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc93178b0dd1f87565c38e3fbc2cf622",
     "grade": false,
     "grade_id": "cell-e836d116ed9b791c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "depth_values = [1, 2, 3, 5, 10]\n",
    "\n",
    "fig, axes_matrix = plt.subplots(nrows=len(depth_values), ncols=2,\n",
    "                                figsize=(2*5, 5*len(depth_values)))\n",
    "for depth, (axes_train, axes_test) in zip(depth_values, axes_matrix):\n",
    "    # The boilerplate code is the elegant way to get a grid of plots in matplotlib\n",
    "    # All you need is to fit a decision tree and call plot_decision_surface\n",
    "    # axes_train and axes_test for train and test plots correspondingly\n",
    "    # your code here\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (difficulty: moderate)\n",
    "We need a better tree!\n",
    "\n",
    "Try adjusting the parameters of DecisionTreeClassifier to improve the test accuracy for the multiclassification problem.\n",
    "\n",
    "    Accuracy >= 0.72 - not bad for a start\n",
    "    Accuracy >= 0.75 - better, but not enough\n",
    "    Accuracy >= 0.77 - pretty good\n",
    "    Accuracy >= 0.78 - great! (probably the best result for a single tree)\n",
    "\n",
    "Feel free to modify the DecisionTreeClassifier below instead of re-writing everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifer\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_toy_multiclass_train, y_toy_multiclass_train)\n",
    "\n",
    "# Plot the decision surface on the test data\n",
    "# Note the accuracy number in top-right corner\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_decision_surface(clf, X_toy_multiclass_test, y_toy_multiclass_test, cmap='rainbow', grid_step=0.2, axes=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0d58a3791692bd4d4bd56ae43bd8ec4",
     "grade": false,
     "grade_id": "cell-68ab8a1594746e30",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2845972f29c717081b5b0d27d119616d",
     "grade": true,
     "grade_id": "cell-210c56c4a7137d35",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "031965c6a5fffc2f870808a0e8fc6de2",
     "grade": true,
     "grade_id": "cell-bca4f1bc4bf5ada2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6488f762a2d07d65b2ce9311caca86ae",
     "grade": true,
     "grade_id": "cell-9e463e173318a658",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "697187eb32669af123717360e051d572",
     "grade": true,
     "grade_id": "cell-2b850ac185ff9ca3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees conclusion\n",
    "Key advantages:\n",
    "* Interpretable results\n",
    "* Not sensitive to data scaling\n",
    "* Natural support for missing values and categorial data\n",
    "\n",
    "Key disadvanatages:\n",
    "* (comparatively) poor quality of the result\n",
    "* Unstable training - small variations in the data might result in a completely different tree being generated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
