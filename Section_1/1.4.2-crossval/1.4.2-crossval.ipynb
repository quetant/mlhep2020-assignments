{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td><center><img src=\"img/mlhep-logo-transparent.png\" width=\"400\"></center></td>\n",
    "    <td><h1><center>The Sixth Machine Learning in High Energy Physics Summer School (MLHEP) 2020</center></h1></td>\n",
    "  </tr>\n",
    " </table>\n",
    "\n",
    "<h1><center>Seminar</center></h1>\n",
    "<h2><center>Cross-Validation, Quality Metric Uncertainty Estimation, <br>Statistical Model Comparison</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.testing as np_testing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part 1: Quality metric uncertainty estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data preparation\n",
    "\n",
    "UCI MAGIC dataset: https://archive.ics.uci.edu/ml/datasets/magic+gamma+telescope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The data are MC generated (see below) to simulate registration of high energy gamma particles in a ground-based atmospheric Cherenkov gamma telescope using the imaging technique. Cherenkov gamma telescope observes high energy gamma rays, taking advantage of the radiation emitted by charged particles produced inside the electromagnetic showers initiated by the gammas, and developing in the atmosphere. This Cherenkov radiation (of visible to UV wavelengths) leaks through the atmosphere and gets recorded in the detector, allowing reconstruction of the shower parameters. The available information consists of pulses left by the incoming Cherenkov photons on the photomultiplier tubes, arranged in a plane, the camera. Depending on the energy of the primary gamma, a total of few hundreds to some 10000 Cherenkov photons get collected, in patterns (called the shower image), allowing to discriminate statistically those caused by primary gammas (signal) from the images of hadronic showers initiated by cosmic rays in the upper atmosphere (background).\n",
    "\n",
    "Features description:\n",
    "- **Length:** continuous # major axis of ellipse [mm]\n",
    "- **Width:** continuous # minor axis of ellipse [mm]\n",
    "- **Size:** continuous # 10-log of sum of content of all pixels [in #phot]\n",
    "- **Conc:** continuous # ratio of sum of two highest pixels over fSize [ratio]\n",
    "- **Conc1:** continuous # ratio of highest pixel over fSize [ratio]\n",
    "- **Asym:** continuous # distance from highest pixel to center, projected onto major axis [mm]\n",
    "- **M3Long:** continuous # 3rd root of third moment along major axis [mm]\n",
    "- **M3Trans:** continuous # 3rd root of third moment along minor axis [mm]\n",
    "- **Alpha:** continuous # angle of major axis with vector to origin [deg]\n",
    "- **Dist:** continuous # distance from origin to center of ellipse [mm]\n",
    "- **Label:** g,h # gamma (signal), hadron (background)\n",
    "\n",
    "g = gamma (signal): 12332 \\\n",
    "h = hadron (background): 6688"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_names = np.array([\"Length\", \"Width\", \"Size\", \"Conc\", \"Conc1\", \"Asym\", \"M3Long\", \"M3Trans\", \"Alpha\", \"Dist\"])\n",
    "\n",
    "data = pd.read_csv(\"data/MAGIC/magic04.data\", header=None, names=list(f_names)+[\"Label\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a matrix of input features\n",
    "X = data[f_names].values\n",
    "\n",
    "# prepare a vector of true labels\n",
    "y = 1 * (data['Label'].values == \"g\")\n",
    "\n",
    "\n",
    "\n",
    "# scale data (apply to X, y for the further simplisity)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scale data: X = (X - mean) / sigma\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into train and test subsamples to fit and test classifiers\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=11, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Fit a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Logistic Regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define a classifier\n",
    "logreg = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, class_weight=None, solver='lbfgs', random_state=11)\n",
    "\n",
    "# fit it using the train subsample\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for the test subsample\n",
    "y_test_proba = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# predict labels\n",
    "y_test_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Proba: \", y_test_proba[:5])\n",
    "print(\"Pred:  \", y_test_pred[:5])\n",
    "print(\"True:  \", y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Compute quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def quality_metrics_report(y_true, y_pred, y_proba):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: array-like of shape (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred: array-like of shape (n_samples,)\n",
    "        Estimated targets as returned by a classifier.\n",
    "    y_proba : array, shape = [n_samples]\n",
    "        Target scores, can be probability estimates of the positive\n",
    "        class.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List of metric values: [accuracy, precision, recall, f1, roc_auc]\n",
    "    \"\"\"\n",
    "    \n",
    "    accuracy  = metrics.accuracy_score(y_true, y_pred)\n",
    "    precision = metrics.precision_score(y_true, y_pred)\n",
    "    recall    = metrics.recall_score(y_true, y_pred)\n",
    "    f1        = metrics.f1_score(y_true, y_pred)\n",
    "    roc_auc   = metrics.roc_auc_score(y_true, y_proba)\n",
    "    \n",
    "    return [accuracy, precision, recall, f1, roc_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute roc auc score on the test\n",
    "[accuracy, precision, recall, f1, roc_auc] = quality_metrics_report(y_test, y_test_pred, y_test_proba)\n",
    "\n",
    "print(\"Test sample:\")\n",
    "print(\"Accuracy:  \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall:    \", recall)\n",
    "print(\"F1-score:  \", f1)\n",
    "print(\"ROC AUC:   \", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Quality metric uncertainty estimation with Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<center><img src=\"img/bootstrap.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Uncertainty estimation with bootstrap:\n",
    "\n",
    "1. Given a model fitted on a train sample\n",
    "2. ùëÅ ‚Äì number of objects in a test sample\n",
    "3. For ùëñ=1, ‚Ä¶, ùêµ do: \\\n",
    "    3.1 Sample with replacement a subsample with ùëÅ objects from the test sample \\\n",
    "    3.2 Calculate quality metrics on this subsample\n",
    "4. Estimate statistics of the metrics: mean, variance, confidence intervals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 1\n",
    "Estimate the quality metrics uncertainties for the classifier considered above. For this, complete the function below.\n",
    "\n",
    "**Hint:** to sample indeces with replacements use `np.random.choice()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46eda21333a0f659c0ffc9e2e118fcdf",
     "grade": false,
     "grade_id": "85c90a_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def botstrap_uncertainties(model, X_test, y_test, iters=100):\n",
    "    \n",
    "    metrics = []\n",
    "    \n",
    "    for i in range(iters):\n",
    "        \n",
    "        # you need to sample a subsample indeces using np.random.choice():\n",
    "        # inds_boot = ...\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        X_test_boot = X_test[inds_boot]\n",
    "        y_test_boot = y_test[inds_boot]\n",
    "        \n",
    "        # make prediction\n",
    "        y_test_proba_boot = model.predict_proba(X_test_boot)[:, 1]\n",
    "        y_test_pred_boot  = model.predict(X_test_boot)\n",
    "        \n",
    "        # compute quaility metrics\n",
    "        metrics_boot = quality_metrics_report(y_test_boot, y_test_pred_boot, y_test_proba_boot)\n",
    "        metrics.append(metrics_boot)\n",
    "        \n",
    "    metrics = np.array(metrics)\n",
    "    df = pd.DataFrame()\n",
    "    df['Metrics'] = columns=['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC']\n",
    "    df['Mean']    = metrics.mean(axis=0)\n",
    "    df['Std']     = metrics.std(axis=0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = botstrap_uncertainties(logreg, X_test, y_test, iters=100)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Expected output (approximately):\n",
    "\n",
    "<center>   \n",
    "    \n",
    "```python\n",
    "     Metrics      Mean       Std\n",
    "0   Accuracy  0.789478  0.004269\n",
    "1  Precision  0.801227  0.004772\n",
    "2     Recall  0.897766  0.004172\n",
    "3         F1  0.846743  0.003407\n",
    "4    ROC AUC  0.835261  0.004732\n",
    "    \n",
    "``` \n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fa274f4e9d1fed28b467f4e2ad21df7",
     "grade": true,
     "grade_id": "85c90a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part 2: Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## K-Fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<center><img src=\"img/kfold.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "K-Fold:\n",
    "    \n",
    "1. Split the data into ùêæ folds\n",
    "2. For ùëñ=1,‚Ä¶,ùêæ do: \\\n",
    "    2.1 Keep ùëñ-th fold for validation \\\n",
    "    2.2 Use other ùêæ‚àí1 folds to fit a model \\\n",
    "    2.3 Measure its quality on the validation fold \\\n",
    "3. Estimate mean and standard deviation of the quality metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a classifier\n",
    "logreg = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, class_weight=None, solver='lbfgs', random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 2\n",
    "Using K-Fold cross-validation estimate means and standard deviation of the quality metrics for the classifier above. \n",
    "\n",
    "**Hint:** use `sklearn.model_selection.KFold(shuffle=True, random_state=11)` as it is shown in https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html . Use function `quality_metrics_report` above to compute the quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03f2b077efcd595a634b7c6ca4f4d6f5",
     "grade": false,
     "grade_id": "a3e242_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kfold_uncertainties(model, X, y, n_splits=10):\n",
    "    \n",
    "    metrics = []\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=11)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        # fit the model on the train subsample, \n",
    "        # get y_test_proba and y_test_pred predictions on the test\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        # compute quaility metrics\n",
    "        metrics_iter = quality_metrics_report(y[test_index], y_test_pred, y_test_proba)\n",
    "        metrics.append(metrics_iter)\n",
    "        \n",
    "    metrics = np.array(metrics)\n",
    "    df = pd.DataFrame()\n",
    "    df['Metrics'] = columns=['Accuracy', 'Precision', 'Recall', 'F1', 'ROC AUC']\n",
    "    df['Mean']    = metrics.mean(axis=0)\n",
    "    df['Std']     = metrics.std(axis=0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kf = kfold_uncertainties(logreg, X, y, n_splits=10)\n",
    "print(df_kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Expected output:\n",
    "\n",
    "<center>   \n",
    "    \n",
    "```python\n",
    "     Metrics      Mean       Std\n",
    "0   Accuracy  0.790694  0.006743\n",
    "1  Precision  0.802153  0.010757\n",
    "2     Recall  0.898820  0.008051\n",
    "3         F1  0.847678  0.006583\n",
    "4    ROC AUC  0.839133  0.006790\n",
    "    \n",
    "``` \n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85c35f7ad8497b36723a1869cb7ed9e7",
     "grade": true,
     "grade_id": "a3e242",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part 3: Statistical model comparison (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hypothesis test\n",
    "\n",
    "<center><img src=\"img/hypo.png\" width=\"600\"></center>\n",
    "\n",
    "We have two hypothesis:\n",
    "* $ùêª_0$: the models have the same quality\n",
    "* $ùêª_1$: the models have the different qualities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Define models we would like to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model\n",
    "model_1 = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, class_weight=None, solver='lbfgs', random_state=11)\n",
    "\n",
    "# test a model\n",
    "kfold_uncertainties(model_1, X, y, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import kNN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# define a model 2\n",
    "model_2 = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# test a model\n",
    "kfold_uncertainties(model_2, X, y, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install mlxtend lib\n",
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Dietterich‚Äôs 5x2-Fold CV paired t test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "\n",
    "t, p = paired_ttest_5x2cv(estimator1=model_1,\n",
    "                          estimator2=model_2,\n",
    "                          X=X, y=y,\n",
    "                          random_seed=1)\n",
    "\n",
    "print('t statistic: %.3f' % t)\n",
    "print('p-value: %.3f' % p)\n",
    "\n",
    "if p <= 0.05:\n",
    "    print(\"The models are significantply different.\")\n",
    "else:\n",
    "    print(\"The models are NOT significantply different.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
