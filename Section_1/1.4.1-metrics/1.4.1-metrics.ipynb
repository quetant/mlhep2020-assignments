{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td><center><img src=\"img/mlhep-logo-transparent.png\" width=\"400\"></center></td>\n",
    "    <td><h1><center>The Sixth Machine Learning in High Energy Physics Summer School (MLHEP) 2020</center></h1></td>\n",
    "  </tr>\n",
    " </table>\n",
    "\n",
    "<h1><center>Seminar</center></h1>\n",
    "<h2><center>Quality metrics for regression and classification</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.testing as np_testing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part 1: Classification quality metrics\n",
    "\n",
    "We will consider a range of classification quality metrics solving a classification problem using several classifiers. We will compare these classifiers and select the best one for a particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Read Data\n",
    "\n",
    "We are going to consider a dataset from a [UCI HTRU2](https://archive.ics.uci.edu/ml/datasets/HTRU2). \n",
    "\n",
    "\tHTRU2 is a data set which describes a sample of pulsar candidates collected during the\n",
    "\tHigh Time Resolution Universe Survey (South). \n",
    "\t\n",
    "\tPulsars are a rare type of Neutron star that produce radio emission detectable here on\n",
    "\tEarth. They are of considerable scientific interest as probes of space-time, the inter-\n",
    "\tstellar medium, and states of matter (see [2] for more uses). \n",
    "\t\n",
    "\tAs pulsars rotate, their emission beam sweeps across the sky, and when this crosses\n",
    "\tour line of sight, produces a detectable pattern of broadband radio emission. As pulsars\n",
    "\trotate rapidly, this pattern repeats periodically. Thus pulsar search involves looking\n",
    "\tfor periodic radio signals with large radio telescopes.\n",
    "\t\n",
    "\tEach pulsar produces a slightly different emission pattern, which varies slightly with each\n",
    "\trotation (see [2] for an introduction to pulsar astrophysics to find out why). Thus a \n",
    "\tpotential signal detection known as a 'candidate', is averaged over many rotations of the\n",
    "\tpulsar, as determined by the length of an observation. In the absence of additional info,\n",
    "\teach candidate could potentially describe a real pulsar. However in practice almost all\n",
    "\tdetections are caused by radio frequency interference (RFI) and noise, making legitimate\n",
    "\tsignals hard to find.\n",
    "    \n",
    "\tThe data set shared here contains 16,259 spurious examples caused by RFI/noise, and 1,639\n",
    "\treal pulsar examples. These examples have all been checked by human annotators. Each\n",
    "\tcandidate is described by 8 continuous variables. The first four are simple statistics\n",
    "\tobtained from the integrated pulse profile (folded profile). This is an array of continuous\n",
    "\tvariables that describe a longitude-resolved version of the signal that has been averaged\n",
    "\tin both time and frequency (see [3] for more details). The remaining four variables are\n",
    "\tsimilarly obtained from the DM-SNR curve (again see [3] for more details).\n",
    "\n",
    "Dataset description:\n",
    "\n",
    "* **MeanInt:** Mean of the integrated profile.\n",
    "* **StdInt:** Standard deviation of the integrated profile.\n",
    "* **ExcessInt:** Excess kurtosis of the integrated profile.\n",
    "* **SkewnessInt:** Skewness of the integrated profile.\n",
    "* **MeanDM:** Mean of the DM-SNR curve.\n",
    "* **StdDM:** Standard deviation of the DM-SNR curve.\n",
    "* **ExcessDM:** Excess kurtosis of the DM-SNR curve.\n",
    "* **SkewnessDM:** Skewness of the DM-SNR curve.\n",
    "* **Label:** The class labels used are 0 (negative) and 1 (positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define column names\n",
    "col_names = ['MeanInt', 'StdInt', 'ExcessInt', 'SkewnessInt', \n",
    "             'MeanDM', 'StdDM', 'ExcessDM', 'SkewnessDM', 'Label']\n",
    "\n",
    "# read data\n",
    "data = pd.read_csv('data/HTRU2/HTRU_2.csv', header=None, names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate Label column\n",
    "label_col = data.columns == 'Label'\n",
    "\n",
    "# Take all columns that are not Label\n",
    "X = data.loc[:, ~label_col].values\n",
    "\n",
    "# Take Label column\n",
    "y = data.loc[:, label_col].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Scale input data using StandardScaler:\n",
    "$$\n",
    "X_{new} = \\frac{X - \\mu}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create object of the class and set up its parameters\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Estimate mean and sigma values\n",
    "ss.fit(X_train)\n",
    "\n",
    "# Scale train and test samples\n",
    "X_train = ss.transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fit Classifiers\n",
    "\n",
    "We will consider three classifiers:\n",
    "* kNN\n",
    "* Decision Tree\n",
    "* Logistic Regression\n",
    "\n",
    "We will use scikit-learn implementations of these classifiers. Their descriptions: [kNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "\n",
    "[Example:](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "<center><img src=\"img/clfs.png\" width=\"500\"></center>\n",
    "\n",
    "Let's just import them and fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import kNN classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create an object of the classifier's class\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the classifier\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Decision Tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create an object of the classifier's class\n",
    "dt = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, \n",
    "                            min_samples_split=2, min_samples_leaf=10, class_weight=None, random_state=11)\n",
    "\n",
    "# Fit the classifier\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Logistic Regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create an object of the classifier's class\n",
    "logreg = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, class_weight=None, solver='lbfgs', random_state=11)\n",
    "\n",
    "# Fit the classifier\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Make prediction of target **label**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN\n",
    "y_test_knn = knn.predict(X_test)\n",
    "\n",
    "# Decision Tree\n",
    "y_test_dt = dt.predict(X_test)\n",
    "\n",
    "# Logistic Regression\n",
    "y_test_logreg = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Truth  : \", y_test[:10])\n",
    "print(\"kNN    : \", y_test_knn[:10])\n",
    "print(\"DT     : \", y_test_dt[:10])\n",
    "print(\"LogReg : \", y_test_logreg[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Make prediction of **probability** of the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN\n",
    "y_test_proba_knn = knn.predict_proba(X_test)[:, 1] # probability of positive response\n",
    "\n",
    "# Decision Tree\n",
    "y_test_proba_dt = dt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Logistic Regression\n",
    "y_test_proba_logreg = logreg.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Truth  : \", y_test[:10])\n",
    "print(\"kNN    : \", y_test_proba_knn[:10])\n",
    "print(\"DT     : \", y_test_proba_dt[:10])\n",
    "print(\"LogReg : \", y_test_proba_logreg[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Label-based Quality Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Consider a confusion matrix:\n",
    "\n",
    "<center><img src='img/binary_conf.png'></center>\n",
    "\n",
    "* TP (true positive) - currectly predicted positives\n",
    "* FP (false positive) - incorrectly predicted negatives (1st order error)\n",
    "* FN (false negative) - incorrectly predicted positives (2nd order error)\n",
    "* TN (true negative) - currectly predicted negatives\n",
    "* Pos (Neg) - total number of positives (negatives)\n",
    "\n",
    "Quality metrics:\n",
    "\n",
    "* $ \\text{Accuracy} = \\frac{TP + TN}{Pos+Neg}$\n",
    "* $ \\text{Error rate} = 1 -\\text{accuracy}$\n",
    "* $ \\text{Precision} =\\frac{TP}{TP + FP}$ \n",
    "* $ \\text{Recall} =\\frac{TP}{TP + FN} = \\frac{TP}{Pos}$\n",
    "* $ \\text{F}_\\beta \\text{-score} = (1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\cdot \\mathrm{precision}) + \\mathrm{recall}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 1\n",
    "Complete a function that computes TP, FP, TN, FN, Accuracy, Error rate, Precision, Recall and F1-score metrics for a classifier.\n",
    "\n",
    "**Hint:** use implementation of the metrics from `sklearn.metrics` as it is shown below. Example for confusin matrix: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebbf810a9be4b987c7b4ab3dd3799c4a",
     "grade": false,
     "grade_id": "9454d8_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def quality_metrics_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: array-like of shape (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred: array-like of shape (n_samples,)\n",
    "        Estimated targets as returned by a classifier.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List of metric values: [tp, fp, fn, tn, accuracy, error_rate, precision, recall, f1]\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return [tp, fp, fn, tn, accuracy, error_rate, precision, recall, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_metrics_report([0, 1, 0, 1], [1, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Expected output:\n",
    "\n",
    "<center>   \n",
    "    \n",
    "```python\n",
    "[1, 2, 1, 0, 0.25, 0.75, 0.3333333333333333, 0.5, 0.4]\n",
    "    \n",
    "``` \n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b2279d0deb4fea80b1b25f35ed923b70",
     "grade": true,
     "grade_id": "9454d8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's compute all these quality metrics for all classifiers considered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_report = pd.DataFrame(columns=['TP', 'FP', 'FN', 'TN', 'Accuracy', 'Error rate', 'Precision', 'Recall', 'F1'])\n",
    "\n",
    "metrics_report.loc['kNN', :] = quality_metrics_report(y_test, y_test_knn)\n",
    "metrics_report.loc['DT', :] = quality_metrics_report(y_test, y_test_dt)\n",
    "metrics_report.loc['LogReg', :] = quality_metrics_report(y_test, y_test_logreg)\n",
    "\n",
    "metrics_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Probability-based Quality Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ROC curve\n",
    "\n",
    "The receiver operating characteristic curve (ROC) measures how well a classifier separates two classes. \n",
    "\n",
    "Let $y_{\\rm i}$ is a true label and $\\hat{y}_{\\rm i}$ is a predicted score for the $i^{\\rm th}$ observation. \n",
    "\n",
    "The numbers of positive and negative observations: $\\mathcal{I}_{\\rm 1} = \\{i: y_{\\rm i}=1\\}$ and $\\mathcal{I}_{\\rm 0} = \\{i: y_{\\rm i}=0\\}$. \n",
    "\n",
    "The sum of observation weights for each class: $W_{\\rm 1} = \\sum_{i \\in \\mathcal{I}_{\\rm 1}} w_{\\rm i}$ and  $W_{\\rm 0} = \\sum_{i \\in \\mathcal{I}_{\\rm 0}} w_{\\rm i}$. \n",
    "\n",
    "For each predicted score threshold value $\\tau$, True Positive Rate (TPR) and False Positive Rate (FPR) are calculated:\n",
    "\n",
    "\\begin{equation}\n",
    "TPR(\\tau) = \\frac{1}{W_{\\rm 1}} \\sum_{i \\in \\mathcal{I}_{\\rm 1}} I[\\hat{y}_{\\rm i} \\ge \\tau] w_{\\rm i}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "FPR(\\tau) = \\frac{1}{W_{\\rm 0}} \\sum_{i \\in \\mathcal{I}_{\\rm 0}} I[\\hat{y}_{\\rm i} \\ge \\tau] w_{\\rm i}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 2\n",
    "Complete the fucntion below, that computes a ROC curve and ROC AUC for a classifier.\n",
    "\n",
    "**Hint:** use `roc_curve` and `auc` from `from sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93e6d66e0c99272e064263f51db00e2a",
     "grade": false,
     "grade_id": "7dc45a_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def roc_curve_report(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: array-like of shape (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "    y_proba: array-like of shape (n_samples,)\n",
    "        Predicted probabilities of the positive class predicted by a classifier.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    fpr : array, shape = [>2]\n",
    "        Increasing false positive rates such that element i is the false\n",
    "        positive rate of predictions with score >= thresholds[i].\n",
    "    tpr : array, shape = [>2]\n",
    "        Increasing true positive rates such that element i is the true\n",
    "        positive rate of predictions with score >= thresholds[i].\n",
    "    roc_auc : float\n",
    "        Area under the ROC curve defined by the fpr and tpr.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve_report([0, 1, 0, 1], [0.6, 0.9, 0.1, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Expected output:\n",
    "\n",
    "<center>   \n",
    "    \n",
    "```python\n",
    "(array([0. , 0. , 0.5, 0.5, 1. ]), array([0. , 0.5, 0.5, 1. , 1. ]), 0.75)\n",
    "    \n",
    "``` \n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b02f739b535fa96a2e25cd08985d79e",
     "grade": true,
     "grade_id": "7dc45a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's plot ROC curves for all classifiers considered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr_knn, tpr_knn, auc_knn          = roc_curve_report(y_test, y_test_proba_knn)\n",
    "fpr_dt, tpr_dt, auc_dt             = roc_curve_report(y_test, y_test_proba_dt)\n",
    "fpr_logreg, tpr_logreg, auc_logreg = roc_curve_report(y_test, y_test_proba_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(fpr_knn, tpr_knn, linewidth=3, label='kNN')\n",
    "plt.plot(fpr_dt, tpr_dt, linewidth=3, label='DT')\n",
    "plt.plot(fpr_logreg, tpr_logreg, linewidth=3, label='LogReg')\n",
    "\n",
    "plt.xlabel('FPR', size=18)\n",
    "plt.ylabel('TPR', size=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=14)\n",
    "plt.grid(b=1)\n",
    "plt.show()\n",
    "\n",
    "print('kNN ROC AUC    :', auc_knn)\n",
    "print('DT ROC AUC     :', auc_dt)\n",
    "print('LogReg ROC AUC :', auc_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Precision-Recall curve\n",
    "\n",
    "The same idea as for ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "precision_knn, recall_knn, _ = precision_recall_curve(y_test, y_test_proba_knn)\n",
    "#auc_knn = auc(fpr_knn, tpr_knn)\n",
    "\n",
    "precision_dt, recall_dt, _ = precision_recall_curve(y_test, y_test_proba_dt)\n",
    "#auc_dt = auc(fpr_dt, tpr_dt)\n",
    "\n",
    "precision_logreg, recall_logreg, _ = precision_recall_curve(y_test, y_test_proba_logreg)\n",
    "#auc_logreg = auc(fpr_logreg, tpr_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(recall_knn, precision_knn, linewidth=3, label='kNN')\n",
    "plt.plot(recall_dt, precision_dt, linewidth=3, label='DT')\n",
    "plt.plot(recall_logreg, precision_logreg, linewidth=3, label='LogReg')\n",
    "\n",
    "plt.xlabel('Recall', size=18)\n",
    "plt.ylabel('Precision', size=18)\n",
    "\n",
    "plt.legend(loc='best', fontsize=14)\n",
    "plt.grid(b=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Questions**:\n",
    "* Which classifier is better?\n",
    "* How can you improve the quality of these models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Part 2: Regression Quality Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "X = np.linspace(0, 6, 200)[:, np.newaxis]\n",
    "y = np.sin(X).ravel() + np.sin(6 * X).ravel() + np.random.RandomState(1).normal(0, 0.1, X.shape[0]) + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(X[:, 0], y)\n",
    "\n",
    "plt.xlabel('X', size=18)\n",
    "plt.ylabel('y', size=18)\n",
    "plt.grid(b=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train and test samples\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Fit Regressors\n",
    "\n",
    "You have learnt three regressors:\n",
    "* kNN\n",
    "* Decision Tree\n",
    "* Linear Regression\n",
    "\n",
    "We will use scikit-learn implementation of these regressors. Their descriptions: [kNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html), [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html), [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "Let's just import them and fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import kNN regressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Create object of the regressor's class\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=10)\n",
    "\n",
    "# Fit the regressor\n",
    "knn_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Decision Tree regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create object of the regressor's class\n",
    "dt_reg = DecisionTreeRegressor(criterion='mse', splitter='best', max_depth=4, \n",
    "                            min_samples_split=2, min_samples_leaf=1, random_state=11)\n",
    "\n",
    "# Fit the regressor\n",
    "dt_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Linear Regression regressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create object of the regressor's class\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# Fit the regressor\n",
    "linreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN\n",
    "y_test_knn_reg = knn_reg.predict(X_test)\n",
    "\n",
    "# DT\n",
    "y_test_dt_reg = dt_reg.predict(X_test)\n",
    "\n",
    "# LinReg\n",
    "y_test_linreg = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.scatter(X_test[:, 0], y_test, color='0', label='Truth')\n",
    "\n",
    "sortd_inds = np.argsort(X_test[:, 0])\n",
    "plt.plot(X_test[sortd_inds, 0], y_test_knn_reg[sortd_inds], linewidth=3, color='b', label='kNN')\n",
    "plt.plot(X_test[sortd_inds, 0], y_test_dt_reg[sortd_inds], linewidth=3, color='r', label='DT')\n",
    "plt.plot(X_test[sortd_inds, 0], y_test_linreg[sortd_inds], linewidth=3, color='g', label='LinReg')\n",
    "\n",
    "plt.xlabel('X', size=18)\n",
    "plt.ylabel('y', size=18)\n",
    "plt.legend(loc='best', fontsize=14)\n",
    "plt.grid(b=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Questions**:\n",
    "* How can you explain behavior of the regressors?\n",
    "* What will happen if one changes their hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Quality Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**1. (R)MSE ((Root) Mean Squared Error)**\n",
    "\n",
    "$$ L(\\hat{y}, y) = \\frac{1}{N}\\sum\\limits_{n=1}^N (y_n - \\hat{y}_n)^2$$\n",
    "\n",
    "**2. MAE (Mean Absolute Error)**\n",
    "\n",
    "$$ L(\\hat{y}, y) = \\frac{1}{N}\\sum\\limits_{n=1}^N |y_n - \\hat{y}_n|$$\n",
    "\n",
    "**3. RSE (Relative Squared Error)**\n",
    "\n",
    "$$ L(\\hat{y}, y) = \\sqrt\\frac{\\sum\\limits_{n=1}^N (y_n - \\hat{y}_n)^2}{\\sum\\limits_{n=1}^N (y_n - \\bar{y})^2}$$\n",
    "\n",
    "**4. RAE (Relative Absolute Error)**\n",
    "\n",
    "$$ L(\\hat{y}, y) = \\frac{\\sum\\limits_{n=1}^N |y_n - \\hat{y}_n|}{\\sum\\limits_{n=1}^N |y_n - \\bar{y}|}$$\n",
    "\n",
    "**5. MAPE (Mean Absolute Persentage Error)**\n",
    "\n",
    "$$ L(\\hat{y}, y) = \\frac{100}{N} \\sum\\limits_{n=1}^N\\left|\\frac{ y_n - \\hat{y}_n}{y_n}\\right|$$\n",
    "\n",
    "\n",
    "**6. RMSLE (Root Mean Squared Logarithmic Error)**\n",
    "\n",
    "$$ L(\\hat{y}, y) = \\sqrt{\\frac{1}{N}\\sum\\limits_{n=1}^N(\\log(y_n + 1) - \\log(\\hat{y}_n + 1))^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 3\n",
    "Complete the function below that computes mse, mae, rse, rae, mape, rmsle metrics for a regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee1441d7aed0a6447d7292c1ccd95064",
     "grade": false,
     "grade_id": "22a8f7_answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error\n",
    "\n",
    "def regression_quality_metrics_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: array-like of shape (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred: array-like of shape (n_samples,)\n",
    "        Estimated targets as returned by a regressor.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List of metric values: [rmse, mae, rse, rae, mape, rmsle]\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    rmse  = np.sqrt( mean_squared_error(y_true, y_pred) )\n",
    "    mae   = mean_absolute_error(y_true, y_pred)\n",
    "    rmsle = np.sqrt( mean_squared_log_error(y_true, y_pred) )\n",
    "    \n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return [rmse, mae, rse, rae, mape, rmsle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_quality_metrics_report(y_test, y_test_linreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Expected output:\n",
    "\n",
    "<center>   \n",
    "    \n",
    "```python\n",
    "[0.8076899021597924,\n",
    " 0.6851055642060055,\n",
    " 0.785078577131241,\n",
    " 0.8107258374023686,\n",
    " 29.634749024687267,\n",
    " 0.22301402015722577]\n",
    "    \n",
    "``` \n",
    "    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a775e0fa518fa78958fa962baa677e02",
     "grade": true,
     "grade_id": "22a8f7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's compute all these metrics for all regressors considered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_report = pd.DataFrame(columns=['RMSE', 'MAE', 'RSE', 'RAE', 'MAPE', 'RMSLE'])\n",
    "\n",
    "metrics_report.loc['kNN', :] = regression_quality_metrics_report(y_test, y_test_knn_reg)\n",
    "metrics_report.loc['DT', :] = regression_quality_metrics_report(y_test, y_test_dt_reg)\n",
    "metrics_report.loc['LinReg', :] = regression_quality_metrics_report(y_test, y_test_linreg)\n",
    "\n",
    "metrics_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Questions**:\n",
    "* Which regressor is better?\n",
    "* How can you improve the quality of these models??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
